{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses XGBOOST for regression.<br>\n",
    "The algorithm is retrained at every step to predict the next test observation. <br>\n",
    "The breakpoint detection is done for every test point.<br>\n",
    "Concept indicators are included as dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import time\n",
    "import csv\n",
    "import functions\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for calculating *Symmetric Mean Absolute Percentage Error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(predictions, actual):\n",
    "    difference = np.abs(predictions-actual)\n",
    "    summation = np.abs(actual)+np.abs(predictions)\n",
    "    error = np.mean(difference/summation)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we can't use *ada_preprocessing()* to extract lagged values and concept, because it can potentially mess up breakpoint detection, we need separate function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_preprocessing(values, pattern):\n",
    "    #receives the list of values up until and including the test point\n",
    "    \n",
    "    #extract the column names\n",
    "    columns = list(pattern.columns)\n",
    "    \n",
    "    #append lagged values\n",
    "    data = [values[-1], values[-2], values[-3], values[-4], values[-5], values[-6]]\n",
    "    \n",
    "    #append the same concept dummies as from the last training point\n",
    "    for i in range(1, list(pattern.columns)[-1]+1):\n",
    "        data.append(int(pattern.loc[:,i]))\n",
    "    \n",
    "    #create the dataframe\n",
    "    df = pd.DataFrame(columns=columns, data=[data])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_forecast(train, test_X):\n",
    "    train_X, train_y = train.iloc[:,1:], train.iloc[:,0]\n",
    "    \n",
    "    model = XGBRegressor(objective = 'reg:squarederror', n_estimators = 100, random_state = 40)\n",
    "    model.fit(train_X, train_y)\n",
    "    yhat = model.predict(test_X)\n",
    "    \n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    n = len(data)\n",
    "    \n",
    "    #transforming concept and transition columns\n",
    "    one_hot_encoded = pd.get_dummies(data[\"concept\"])\n",
    "    data.drop(\"concept\", axis = 1, inplace = True)\n",
    "    data = pd.concat([data, one_hot_encoded], axis = 1, sort = False)\n",
    "    \n",
    "    #for now i'm dropping these columns, cause there was no improvement in accuracy\n",
    "    #by including them so far\n",
    "    data.drop([\"transition\", \"steps_to_bkp\", \"steps_since_bkp\"], axis = 1, inplace = True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_save(predictions, actual, bkp, name):\n",
    "    plt.plot(actual, label = \"Expected\", color = \"black\")\n",
    "    plt.plot(predictions, label = \"Predicted\", color = \"red\")\n",
    "    plt.legend()\n",
    "    \n",
    "    #saving the plots\n",
    "    image_path = name+\".png\"\n",
    "    plt.savefig(image_path)\n",
    "    \n",
    "    \n",
    "    #retrieve rows with breakpoints\n",
    "    bkps = []\n",
    "    for i in bkp.unique()[1:]:\n",
    "        bkps.append(np.where(bkp == i)[0][0])\n",
    "\n",
    "    plt.vlines(x = bkps, ymin = actual.min(), ymax = actual.max(), \n",
    "               linestyles = \"dashed\", color = \"deepskyblue\", label = \"Breakpoints\")\n",
    "    plt.legend()\n",
    "    image_path = name+\"_breakpoints.png\"\n",
    "    plt.savefig(image_path)\n",
    "    \n",
    "    bkp_path = name+\"_breakpoints.txt\"\n",
    "    with open(bkp_path, 'w') as file:\n",
    "        file.write(json.dumps(\"\".join([str(j) for j in bkps])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names = [\"linear1_abrupt\", \"linear2_abrupt\", \"linear3_abrupt\",\n",
    "                \"nonlinear1_abrupt\", \"nonlinear2_abrupt\", \"nonlinear3_abrupt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to store the overall error\n",
    "smape_dict = {}\n",
    "\n",
    "for name in list_of_names:\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    #loading the data\n",
    "    data = pd.read_csv(\"data/\"+name, usecols = [0]).iloc[:,0].to_list()\n",
    "    \n",
    "    #70/30 train/test split\n",
    "    split = int(0.7*len(data))\n",
    "    train, test = data[:split], data[split:]\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        #get breakpoints for train\n",
    "        history = functions.ada_preprocessing(train)\n",
    "        \n",
    "        #save the final set of breakpoints\n",
    "        bkp = None\n",
    "        if i == len(test)-1:\n",
    "            bkp = history[\"concept\"]\n",
    "            \n",
    "        history = one_hot_encoding(history)\n",
    "        \n",
    "        #add new test observation to train series\n",
    "        train.append(test[i])\n",
    "        \n",
    "        #path the last point from history dataframe to then extract same concept dummies\n",
    "        test_df = manual_preprocessing(train, history.tail(1))\n",
    "        \n",
    "        ground_truth.append(train[-1])\n",
    "        \n",
    "        #training data = history\n",
    "        prediction = xgboost_forecast(history, test_df.loc[:,\"t-1\":])\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "            \n",
    "    end = time.perf_counter()\n",
    "    print(\"Time spent: {:.2f}m\".format((end-start)/60))\n",
    "    \n",
    "    error = smape(np.asarray(predictions), np.asarray(ground_truth))\n",
    "    smape_dict[name] = error\n",
    "    print(\"SMAPE: {:.4f}\".format(error))\n",
    "    plot_save(predictions, ground_truth, bkp, \"forecasting_results/xgboost_refedine/\"+name)\n",
    "    \n",
    "dict_path = \"forecasting_results/xgboost_redefine/error.txt\"\n",
    "with open(dict_path, 'w') as file:\n",
    "    file.write(json.dumps(smape_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
