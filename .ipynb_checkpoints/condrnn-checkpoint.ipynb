{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses Recurrent Neural Network with Long-Short-Term Memory units, which takes lagged values as input.<br>\n",
    "Additionally, we pass the **concept number** as a condition. Not dummy encoding because we don't know the total number of concepts in advance.<br>\n",
    "The model is **trained only once** on the training dataset. <br>\n",
    "For each new test point, we first perform the breakpoint detection and only then pass it to the algorithm for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from cond_rnn import ConditionalRNN\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for calculating *Symmetric Mean Absolute Percentage Error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(predictions, actual):\n",
    "    difference = np.abs(predictions-actual)\n",
    "    summation = np.abs(actual)+np.abs(predictions)\n",
    "    error = np.mean(difference/summation)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we can't use *ada_preprocessing()* to extract lagged values and concept, because it can potentially mess up breakpoint detection, we need separate function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_preprocessing(values, pattern):\n",
    "    #receives the list of values up until and including the test point\n",
    "    \n",
    "    columns = list(pattern.columns)\n",
    "    data = [values[-1], values[-2], values[-3], values[-4], values[-5], values[-6], pattern.loc[:,\"concept\"]]\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns, data=[data])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the model, some input preprocessing is due."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_preprocessing(train, test):\n",
    "    train_X, train_y = train.iloc[:,1:], train.iloc[:,0]\n",
    "    test_X, test_y = test.iloc[:,1:], test.iloc[:,0]\n",
    "    \n",
    "    #separate both train and test sets into inputs and auxiliary variables\n",
    "    train_X_input = train_X.loc[:,\"t-1\":\"t-5\"]\n",
    "    train_X_aux = train_X.loc[:,\"concept\"]\n",
    "    test_X_input = test_X.loc[:,\"t-1\":\"t-5\"]\n",
    "    test_X_aux = test_X.loc[:,\"concept\"]\n",
    "    \n",
    "    #now also need to reshape X_input and X_aux\n",
    "    X_arrays = np.asarray(train_X_input)\n",
    "    train_X_input = np.hstack(X_arrays).reshape(train_X_input.shape[0], 1, train_X_input.shape[1])\n",
    "\n",
    "    X_arrays = np.asarray(train_X_aux)\n",
    "    train_X_aux = np.hstack(X_arrays).reshape(train_X_aux.shape[0], 1)\n",
    "    \n",
    "    #need to do the same for test set\n",
    "    X_arrays = np.asarray(test_X_input)\n",
    "    test_X_input = np.hstack(X_arrays).reshape(test_X_input.shape[0], 1, test_X_input.shape[1])\n",
    "\n",
    "    X_arrays = np.asarray(test_X_aux)\n",
    "    test_X_aux = np.hstack(X_arrays).reshape(test_X_aux.shape[0], 1)\n",
    "    \n",
    "    return train_X_input, train_X_aux, test_X_input, test_X_aux, train_y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*outputs = cond_rnn.ConditionalRNN(units=NUM_CELLS, cell='GRU')([inputs, cond])*\n",
    "\n",
    "The conditional RNN expects those parameters:\n",
    "\n",
    "*units*: int, The number of units in the RNN Cell. <br>\n",
    "*cell*: string, cell class or object (pre-instantiated). In the case of string, 'GRU', 'LSTM' and 'RNN' are supported.<br>\n",
    "*inputs*: 3-D Tensor with shape [batch_size, timesteps, input_dim].<br>\n",
    "*cond*: 2-D Tensor or list of tensors with shape [batch_size, cond_dim]. In the case of a list, the tensors can have a different cond_dim.<br>\n",
    "*\\*args, \\**kwargs*: Any parameters of the tf.keras.layers.RNN class, such as return_sequences, return_state, stateful, unroll..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cond_rnn(X_input, X_aux, train_y):\n",
    "    inputs = Input(shape=(X_input.shape[1], X_input.shape[2]), dtype = tf.float32)\n",
    "    cond1 = Input(shape = (X_aux.shape[1]), dtype = tf.float32)\n",
    "    \n",
    "    #building model steps\n",
    "    A = ConditionalRNN(4, cell='LSTM')([inputs, cond1])\n",
    "    out = Dense(1)(A)\n",
    "    model = Model(inputs=[inputs, cond1], outputs=out)\n",
    "    model.compile(loss = \"mean_squared_error\", optimizer = \"adam\")\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',  patience=5, verbose=0, mode='auto',\n",
    "    )\n",
    "    \n",
    "    #fitting the model\n",
    "    model.fit([X_input, X_aux], train_y, epochs = 50, batch_size = 1, callbacks = [es], verbose = 0, shuffle = False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical features, LSTMs expect data to be within the scale of the activation function.<br>\n",
    "Since default activation function for LSTMs is *tanh*, we need to scale our values to be between -1 and 1, which is done by using MinMaxScaler.<br>\n",
    "*The same min and max values should be used for both train and test sets to ensure the fairness of experiment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional RNN model expectes inputs to be numerical time series values.<br>\n",
    "The values that indicate the concept should be one-hot encoded and feed into the Conditional RNN as \"condition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function does one-hot encoding, scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    #so far these columns didn't improve the analysis, so just drop them\n",
    "    data.drop([\"transition\", \"steps_to_bkp\", \"steps_since_bkp\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #scaling\n",
    "    scaler_x = MinMaxScaler(feature_range = (0,1))\n",
    "    data.loc[:,\"t-1\":\"t-5\"] = scaler_x.fit_transform(data.loc[:,\"t-1\":\"t-5\"])\n",
    "    \n",
    "    #need separate scaler only for target\n",
    "    scaler_y = MinMaxScaler(feature_range = (0,1))\n",
    "    data.loc[:,\"t\"] = scaler_y.fit_transform(np.asarray(data.loc[:,\"t\"]).reshape([-1,1]))\n",
    "    \n",
    "    return data, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names = [\"linear1_abrupt\", \"linear2_abrupt\", \"linear3_abrupt\",\n",
    "                \"nonlinear1_abrupt\", \"nonlinear2_abrupt\", \"nonlinear3_abrupt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smape_dict = {}\n",
    "error1 = []\n",
    "error2 = []\n",
    "\n",
    "for name in [list_of_names[0]]:\n",
    "    file_path = \"data/\"+name+\"_series\"\n",
    "    data = pd.read_csv(file_path).iloc[:,0].to_list()\n",
    "\n",
    "    #70/30 train/test split\n",
    "    split = int(0.7*len(data))\n",
    "    train, test = data[:split], data[split:]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    #need to perform the first step outside of the loop,\n",
    "    #because we don't want to retrain our model each time\n",
    "    \n",
    "    #get breakpoints for train dataset\n",
    "    history = functions.ada_preprocessing(train)\n",
    "    history.drop([\"transition\", \"steps_to_bkp\", \"steps_since_bkp\"], axis = 1, inplace = True)\n",
    "\n",
    "    #get the dataframe for new test observation\n",
    "    train.append(test[0])\n",
    "    test_df = manual_preprocessing(train, history.tail(1))\n",
    "    \n",
    "    #change train and test into form appropriate for CondRNN\n",
    "    train_X_input, train_X_aux, test_X_input, test_X_aux, train_y, test_y = forecast_preprocessing(history, test_df)\n",
    "    \n",
    "    model = fit_cond_rnn(train_X_input, train_X_aux, train_y)\n",
    "    \n",
    "    #get predictions for new test observation\n",
    "    prediction = model.predict([test_X_input, test_X_aux])\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "        \n",
    "    #we've got the first prediction, so now start from 1\n",
    "    for i in range(1, len(test)):\n",
    "        #get breakpoints for train dataset\n",
    "        history = functions.ada_preprocessing(train)\n",
    "        \n",
    "        history.drop([\"transition\", \"steps_to_bkp\", \"steps_since_bkp\"], axis = 1, inplace = True)\n",
    "        \n",
    "        #get the dataframe for new test observation\n",
    "        train.append(test[i])\n",
    "        test_row = manual_preprocessing(train, history.tail(1))\n",
    "        \n",
    "        #change train and test into form appropriate for CondRNN\n",
    "        train_X_input, train_X_aux, test_X_input, test_X_aux, train_y, test_y = forecast_preprocessing(history, test_row)\n",
    "\n",
    "        #get predictions for new test observation\n",
    "        prediction = model.predict([test_X_input, test_X_aux])\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        \n",
    "    end = time.perf_counter()\n",
    "    print(\"Time spent: {:.2f}h\".format((end-start)/3600))\n",
    "\n",
    "    error = smape(np.asarray(predictions), np.asarray(test))\n",
    "    smape_dict[name] = error\n",
    "    print(\"SMAPE: {:.4f}\".format(error))\n",
    "    \n",
    "    plt.plot(test, label = \"expected\", color = \"black\")\n",
    "    plt.plot(np.asarray(predictions).reshape([-1,1]), label = \"predicted\", color = \"red\")\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
